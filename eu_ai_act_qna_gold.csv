question,answer
Pourquoi est-il nécessaire de réglementer l'utilisation de l'intelligence artificielle?,"Le règlement de l'UE sur l'intelligence artificielle (IA) est la première législation globale au monde en matière d'IA. Il vise à parer aux risques pour la santé, la sécurité et les droits fondamentaux. Le règlement protège également la démocratie, l'état de droit et l'environnement. Les systèmes d'IA présentent un fort potentiel en matière d'avantages sociétaux, de croissance économique, de stimulation de l'innovation dans l'UE et de renforcement de la compétitivité de l'UE à l'échelle mondiale. Dans certains cas, cependant, les caractéristiques particulières de certains systèmes d'IA peuvent être à l'origine de nouveaux risques pour la sécurité des utilisateurs, y compris leur sécurité physique, et pour les droits fondamentaux. Certains modèles d'IA très puissants dont l'utilisation est très répandue sont même susceptibles de présenter des risques systémiques. Ces risques peuvent être source d'insécurité juridique et potentiellement ralentir l'adoption des technologies de l'IA par les pouvoirs publics, les entreprises et les citoyens, du fait d'un manque de confiance. Or des réponses réglementaires disparates de la part des autorités nationales risqueraient de fragmenter le marché intérieur. Dans ce contexte, afin d'assurer le bon fonctionnement du marché intérieur des systèmes d'IA, il était nécessaire de légiférer en mettant en balance les bénéfices et les risques."
À qui s'applique le règlement sur l'intelligence artificielle?,"Le cadre juridique s'appliquera aux acteurs tant du secteur public que du secteur privé, à l'intérieur comme à l'extérieur de l'UE, dès lors que le système d'IA aura été mis sur le marché dans l'Union ou que son utilisation aura une incidence sur des personnes situées dans l'UE. Les obligations imposées peuvent peser aussi bien sur les fournisseurs (par exemple, le développeur d'un outil d'analyse de CV) que sur les déployeurs de systèmes d'IA (par exemple, une banque qui achète cet outil). Le règlement prévoit des dérogations. Les activités de recherche, de développement et de prototypage qui ont lieu avant la mise sur le marché d'un système d'IA ne sont pas soumises aux dispositions de ce règlement. Ces dernières ne s'appliquent pas non plus aux systèmes d'IA conçus exclusivement à des fins militaires, de défense ou de sécurité nationale, quel que soit le type d'entité exerçant ces activités."
Quelles sont les catégories de risques?,"Le règlement sur l'IA introduit un cadre uniforme applicable dans tous les États membres de l'UE, qui s'articule autour d'une définition prospective de l'IA et d'une approche fondée sur les risques: Risque inacceptable: un ensemble très limité d'utilisations particulièrement néfastes de l'IA qui sont contraires aux valeurs de l'UE parce qu'elles portent atteinte à des droits fondamentaux, et seront donc interdites: l'exploitation des vulnérabilités des personnes, la manipulation et le recours à des techniques subliminales; la notation sociale à des fins publiques et privées; la police prédictive ciblant les individus fondée uniquement sur le profilage des personnes; le moissonnage non ciblé d'images provenant de l'internet ou de la vidéosurveillance afin de constituer ou d'étendre des bases de données de reconnaissance faciale; la reconnaissance des émotions sur le lieu de travail et dans les établissements d'enseignement, sauf pour des raisons médicales ou de sécurité (par exemple, surveillance de l'état de fatigue d'un pilote); la catégorisation biométrique des personnes physiques afin de parvenir à des déductions ou des inférences concernant leur race, leurs opinions politiques, leur affiliation à une organisation syndicale, leurs convictions religieuses ou philosophiques ou leur orientation sexuelle. L'étiquetage ou le filtrage d'ensembles de données et la catégorisation de données dans le domaine répressif demeureront possibles; l'utilisation par les services répressifs de l'identification biométrique à distance en temps réel dans des espaces accessibles au public, sauf exceptions strictement limitées (voir ci-dessous). La Commission publiera des orientations sur les interdictions avant l'entrée en application de ces dernières le 2 février 2025. Risque élevé: un nombre limité de systèmes d'IA définis dans la proposition, qui peuvent avoir une incidence négative sur la sécurité des personnes ou sur leurs droits fondamentaux (tels que protégés par la charte des droits fondamentaux de l'UE), sont considérés comme présentant un risque élevé. Les listes des systèmes d'IA à haut risque, qui pourront être révisées pour qu'il soit tenu compte de l'évolution des cas d'utilisation de l'IA, figurent en annexe du règlement. Ces systèmes d'IA sont notamment des composants de sécurité de produits couverts par une législation sectorielle de l'Union. Ils seront toujours considérés comme étant à haut risque dès lors que, dans le cadre de cette législation sectorielle, ils sont soumis à une évaluation de la conformité par un tiers. Ces systèmes d'IA à haut risque comprennent, par exemple, ceux qui évaluent si une personne est en mesure de recevoir un traitement médical déterminé, d'obtenir un emploi précis ou de se voir accorder un prêt pour acheter un appartement. Entrent également dans la catégorie des systèmes d'IA à haut risque ceux utilisés par la police pour établir le profil de personnes ou évaluer le risque que celles-ci commettent une infraction pénale (à moins qu'ils ne soient interdits par l'article 5). Les systèmes d'IA assurant le fonctionnement de robots, de drones ou de dispositifs médicaux pourraient être aussi à haut risque. Obligations particulières de transparence: afin de renforcer la confiance, il importe de veiller à la transparence en matière d'utilisation de l'IA. C'est pourquoi le règlement sur l'IA instaure des exigences de transparence particulières pour certaines applications d'IA, notamment en cas de risque manifeste de manipulation (par exemple, du fait du recours à des chatbots) ou d'hypertrucages. Les utilisateurs devraient savoir qu'ils interagissent avec une machine. Risque minimal: la majorité des systèmes d'IA peuvent être développés et utilisés à condition de respecter la législation en vigueur et ne sont soumis à aucune obligation légale supplémentaire. Les fournisseurs de ces systèmes peuvent volontairement choisir d'appliquer les exigences relatives à une IA digne de confiance, et adhérer à des codes de conduite volontaires. En outre, le règlement sur l'IA tient compte des risques systémiques que pourraient faire peser des modèles d'IA à usage général, notamment des grands modèles d'IA générative. Ces derniers ont des applications très diverses et constituent de plus en plus souvent la base de nombreux systèmes d'IA dans l'UE. Certains de ces modèles pourraient comporter des risques systémiques s'ils sont très puissants ou si leur utilisation est très répandue. Par exemple, des modèles puissants pourraient provoquer des accidents graves ou être utilisés à mauvais escient pour lancer des cyberattaques de grande ampleur. La propagation, par un modèle, de biais préjudiciables dans de nombreuses applications pourrait avoir des conséquences négatives pour de nombreuses personnes."
Comment savoir si un système d'IA présente un risque élevé?,"Le règlement sur l'IA définit une méthode solide permettant de classer les systèmes d'IA comme étant à haut risque. L'objectif est que les entreprises et les autres opérateurs jouissent d'une sécurité juridique. La classification des risques repose sur la destination du système d'IA, conformément à la législation existante de l'UE en matière de sécurité des produits. Cela signifie que cette classification dépend de la fonction exécutée par le système d'IA, ainsi que du but précis dans lequel ce système est utilisé et des modalités de cette utilisation. Les systèmes d'IA peuvent être classés comme étant à haut risque dans deux cas: si le système d'IA est intégré comme composant de sécurité dans les produits couverts par la législation existante sur les produits (annexe I) ou s'il constitue lui-même un tel produit. Il pourrait s'agir, par exemple, de logiciels médicaux fondés sur l'IA; si le système d'IA est destiné à être utilisé pour un cas d'utilisation à haut risque mentionné à l'annexe III du règlement sur l'IA. La liste figurant à cette annexe énumère des cas d'utilisation dans des domaines tels que l'éducation, l'emploi, la répression ou la migration. La Commission élabore actuellement des lignes directrices pour la classification des systèmes d'IA comme étant à haut risque, qui seront publiées avant la date d'entrée en application de ces règles."
Quels sont les exemples de cas d'utilisation à haut risque tels que définis à l'annexe III?,"L'annexe III, composée de huit domaines dans lesquels l'utilisation de l'IA peut être particulièrement sensible, énumère des cas d'utilisation concrets pour chacun de ces domaines. Un système d'IA est classé comme étant à haut risque s'il est destiné à être utilisé dans l'un de ces cas d'utilisation. Exemples: les systèmes d'IA utilisés comme des composants de sécurité dans certaines infrastructures critiques, par exemple dans les domaines du trafic routier et de l'approvisionnement en eau, gaz, chauffage et électricité; les systèmes d'IA utilisés dans les domaines de l'éducation et de la formation professionnelle, par exemple pour évaluer les acquis d'apprentissage, orienter le processus d'apprentissage et surveiller les comportements malhonnêtes; les systèmes d'IA utilisés dans les domaines de l'emploi, de la gestion de la main- d'œuvre et de l'accès à l'emploi indépendant, par exemple pour placer des offres d'emploi ciblées, pour analyser et filtrer les candidatures et pour évaluer les candidats; les systèmes d'IA utilisés dans le domaine de l'accès aux services privés essentiels et aux services publics et prestations sociales essentiels (tels que les soins de santé), les systèmes utilisés pour évaluer la solvabilité des personnes physiques ainsi que pour évaluer les risques et déterminer la tarification en matière d'assurance-vie et d'assurance maladie; les systèmes d'IA utilisés dans les domaines répressif, migratoire et du contrôle aux frontières, pour autant qu'ils ne soient pas déjà interdits, ainsi que dans ceux de l'administration de la justice et des processus démocratiques; les systèmes d'IA utilisés pour l'identification biométrique, la catégorisation biométrique et la reconnaissance des émotions, pour autant qu'ils ne soient pas interdits."
Quelles sont les obligations des fournisseurs de systèmes d'IA à haut risque?,"Avant qu'un fournisseur puisse mettre un système d'IA à haut risque sur le marché de l'UE ou le mettre en service d'une autre manière, ce système doit faire l'objet d'une évaluation de la conformité. Le fournisseur pourra ainsi démontrer que son système est conforme aux exigences obligatoires relatives à une IA digne de confiance (par exemple, la qualité des données, la documentation et la traçabilité, la transparence, le contrôle humain, l'exactitude, la cybersécurité et la robustesse). Cette évaluation doit être répétée si le système ou sa finalité sont substantiellement modifiés. Les systèmes d'IA qui servent de composants de sécurité de produits couverts par une législation sectorielle de l'UE seront toujours réputés à haut risque dès lors qu'ils sont soumis à une évaluation de la conformité réalisée par un tiers au titre de cette législation sectorielle. En outre, tous les systèmes biométriques, quelle que soit leur application, seront soumis à une évaluation de la conformité par un tiers. Les fournisseurs de systèmes d'IA à haut risque devront également mettre en œuvre des systèmes de gestion de la qualité et des risques afin de veiller à ce qu'ils respectent les nouvelles exigences et de réduire au minimum les risques pour les utilisateurs et les personnes concernées, même après qu'un produit aura été mis sur le marché. Les systèmes d'IA à haut risque déployés par les pouvoirs publics ou par des entités agissant en leur nom devront être enregistrés dans une base de données publique de l'UE, sauf si ces systèmes sont utilisés à des fins répressives et de gestion de la migration. Dans ce dernier cas, ils devront être enregistrés dans une partie non publique de cette base de données qui ne sera accessible qu'aux autorités de contrôle concernées. Afin de veiller à la conformité tout au long du cycle de vie du système d'IA, les autorités de surveillance du marché effectueront des audits réguliers, faciliteront la surveillance après commercialisation et permettront aux fournisseurs de signaler volontairement tout incident grave ou tout manquement aux obligations en matière de droits fondamentaux qui auront été portés à leur attention. Dans des cas exceptionnels, les autorités pourront accorder des dérogations pour que certains systèmes d'IA à haut risque puissent être mis sur le marché. Ces exigences permettront aux autorités nationales de disposer, en cas d'infraction, des informations nécessaires pour déterminer si le système d'IA a été utilisé dans le respect de la législation."
Quel rôle sera dévolu à la normalisation dans le règlement sur l'IA?,"Le règlement sur l'IA imposera des exigences particulières aux systèmes d'IA à haut risque. Les normes européennes harmonisées joueront un rôle essentiel dans la mise en œuvre de ces exigences. En mai 2023, la Commission européenne a mandaté les organisations européennes de normalisation CEN et CENELEC pour élaborer des normes applicables aux exigences concernant les systèmes d'IA à haut risque. Ce mandat va maintenant être modifié pour être mis en conformité avec la version définitive du règlement sur l'IA. Les organisations européennes de normalisation auront jusqu'à la fin du mois d'avril 2025 pour élaborer et publier ces normes. Puis la Commission évaluera ces normes et les avalisera éventuellement, pour les publier au Journal officiel de l'UE. Les systèmes d'IA qui auront été développés conformément aux normes publiées bénéficieront d'une présomption de conformité."
Comment les modèles d'IA à usage général sont-ils réglementés?,"Les modèles d'IA à usage général, notamment les grands modèles d'IA générative, ont des applications très diverses. Certains modèles spécifiques peuvent être intégrés dans de nombreux systèmes d'IA. Il est essentiel que le fournisseur d'un système d'IA intégrant un modèle d'IA à usage général ait accès à toutes les informations nécessaires pour faire en sorte que son système soit sûr et respecte les obligations prévues par le règlement sur l'IA. Par conséquent, ledit règlement oblige les fournisseurs de ces modèles à mettre certaines informations à la disposition des fournisseurs de systèmes en aval. Ces obligations de transparence permettent de mieux comprendre ces modèles. Les fournisseurs de modèles doivent, de surcroît, appliquer des politiques permettant d'assurer qu'ils respectent la législation sur le droit d'auteur lorsqu'ils entraînent leurs modèles. En outre, certains de ces modèles pourraient comporter des risques systémiques en raison de leur puissance ou de la large utilisation qui en est faite. À l'heure actuelle, les modèles d'IA à usage général qui ont été entraînés à l'aide d'une puissance de calcul totale supérieure à 10^25 FLOPS sont considérés comme présentant des risques systémiques. La Commission peut mettre à jour ce seuil, ou le compléter, en fonction des progrès technologiques et peut également désigner d'autres modèles comme présentant des risques systémiques en se fondant sur d'autres critères (par exemple, le nombre d'utilisateurs ou le degré d'autonomie du modèle). Les fournisseurs de modèles présentant des risques systémiques sont tenus d'évaluer et d'atténuer les risques, de signaler les incidents graves, de procéder à des essais et à des évaluations de modèles conformément à l'état de la technique ainsi que d'assurer la cybersécurité de leurs modèles. Les fournisseurs sont invités à collaborer avec le Bureau de l'IA et d'autres parties prenantes afin d'élaborer un code de bonne pratique détaillant les règles et assurant ainsi le développement sûr et responsable de leurs modèles. Ce code devrait constituer un outil central permettant aux fournisseurs de modèles d'IA à usage général de démontrer le respect des obligations qui leur incombent."
Pourquoi le seuil de 10^25 FLOPS est-il approprié pour déterminer les systèmes d'IA à usage général présentant des risques systémiques?,"La quantité de FLOPS est un indicateur des capacités du modèle, et le seuil exact de FLOPS peut être augmenté ou abaissé par la Commission, par exemple en fonction des progrès accomplis dans la mesure objective des capacités des modèles et des évolutions de la puissance de calcul nécessaire pour atteindre un niveau de performance donné. On ne dispose pas encore d'une compréhension suffisante des capacités des modèles situés au- dessus de ce seuil. Ceux-ci pourraient présenter des risques systémiques, aussi est-il raisonnable de soumettre leurs fournisseurs à un ensemble d'obligations supplémentaires."
Quelles obligations relatives au marquage en filigrane et à l'étiquetage des sorties produites par les systèmes d'IA sont énoncées dans le règlement sur l'IA?,"Le règlement sur l'IA institue des règles de transparence pour les contenus produits par l'IA générative afin de lutter contre les risques de manipulation, de tromperie et de désinformation. Il oblige les fournisseurs de systèmes d'IA générative à marquer les sorties produites par leurs systèmes dans un format lisible par machine et à veiller à ce que celles-ci soient identifiables comme ayant été générées ou manipulées par une IA. Les solutions techniques doivent être aussi efficaces, interopérables, solides et fiables que la technologie le permet, compte tenu des spécificités et des limites des différents types de contenus, des coûts de mise en œuvre et de l'état de la technique généralement reconnu, comme cela peut ressortir des normes techniques pertinentes. En outre, les déployeurs de systèmes d'IA générative qui génèrent ou manipulent des images ou des contenus audio ou vidéo constituant des hypertrucages doivent indiquer de manière visible que ces contenus ont été générés ou manipulés par une IA. Les déployeurs d'un système d'IA qui génère ou manipule des textes publiés dans le but d'informer le public sur des questions d'intérêt public doivent également indiquer que le texte a été généré ou manipulé par une IA. Cette obligation ne s'applique pas lorsque le contenu généré par l'IA a fait l'objet d'un processus d'examen humain ou de contrôle éditorial et lorsqu'une personne physique ou morale assume la responsabilité éditoriale de la publication du contenu. Le Bureau de l'IA publiera des lignes directrices pour fournir des orientations supplémentaires aux fournisseurs et aux déployeurs sur les obligations énoncées à l'article 50, qui entreront en application deux ans après l'entrée en vigueur du règlement sur l'IA (soit le 2 août 2026). En outre, le Bureau de l'IA encouragera et facilitera l'élaboration de codes de bonne pratique au niveau de l'Union afin de rationaliser la mise en œuvre effective des obligations relatives à la détection et à l'étiquetage des contenus générés ou manipulés par l'IA. Le règlement sur l'IA est-il adapté aux évolutions futures? Le règlement sur l'IA établit un cadre juridique qui est en phase avec les nouvelles évolutions, s'adapte facilement et rapidement et permet une évaluation fréquente. Le règlement sur l'IA définit des exigences et des obligations axées sur les résultats mais, en ce qui concerne les solutions techniques et la mise en œuvre concrètes, il s'en remet à des normes et à des codes de bonne pratique dictés par l'industrie qui présentent la souplesse nécessaire pour être adaptés aux différents cas d'utilisation et pour permettre l'avènement de nouvelles solutions technologiques. En outre, le règlement lui-même peut être modifié par voie d'actes délégués et d'actes d'exécution, par exemple en vue du réexamen de la liste des cas d'utilisation à haut risque figurant à l'annexe III. Enfin, il sera procédé à des évaluations fréquentes de certaines parties du règlement sur l'IA et, à terme, de celui-ci dans son ensemble, afin de veiller à ce que toute nécessité de révision et de modifications soit mise en évidence."
Comment le règlement sur l'IA régit-il l'identification biométrique?,"Il est interdit d'utiliser, à des fins répressives, l'identification biométrique à distance en temps réel dans des espaces accessibles au public (c'est-à-dire la reconnaissance faciale par des caméras de télévision en circuit fermé). Les États membres peuvent toutefois introduire, par voie législative, des exceptions qui permettraient l'utilisation de l'identification biométrique à distance en temps réel dans les cas suivants: les activités répressives concernant 16 infractions très graves déterminées; la recherche ciblée de certaines victimes, en cas d'enlèvement, de traite et d'exploitation sexuelle d'êtres humains, et de personnes disparues; ou la prévention d'une menace pour la vie ou la sécurité physique de personnes, ou la réaction à une menace actuelle ou prévisible d'attaque terroriste. Toute utilisation exceptionnelle sera subordonnée à une autorisation préalable octroyée par une autorité judiciaire ou une autorité administrative indépendante dont la décision sera contraignante. En cas d'urgence, cette autorisation pourra être délivrée dans les 24 heures; si l'autorisation est refusée, toutes les données et les sorties devront être supprimées. Une analyse d'impact sur les droits fondamentaux devra être réalisée au préalable et être notifiée à l'autorité de surveillance du marché concernée et à l'autorité chargée de la protection des données. En cas d'urgence, il sera possible de commencer à utiliser le système sans enregistrement. L'utilisation de systèmes d'IA pour l'identification biométrique à distance a posteriori (identification de personnes dans du contenu collecté auparavant) de personnes faisant l'objet d'une enquête sera subordonnée à l'autorisation préalable d'une autorité judiciaire ou d'une autorité administrative indépendante ainsi qu'à la notification aux autorités concernées chargées, respectivement, de la protection des données et de la surveillance du marché."
Pourquoi faut-il des règles particulières pour encadrer l'identification biométrique à distance?,"L'identification biométrique peut prendre différentes formes. L'authentification et la vérification biométriques, par exemple pour déverrouiller un smartphone ou dans le cadre de la vérification/de l'authentification aux points de passage frontaliers pour vérifier que l'identité d'une personne correspond à celle indiquée sur ses documents de voyage (comparaison «un à un»), demeurent non réglementées car elles ne présentent pas de risque substantiel pour les droits fondamentaux. À l'inverse, l'identification biométrique, qui peut également être utilisée à distance, par exemple pour identifier des personnes dans une foule, peut avoir une incidence considérable sur la vie privée dans l'espace public. L'exactitude des résultats des systèmes de reconnaissance faciale peut être fortement influencée par toute une série de facteurs, tels que la qualité de la caméra, la lumière, la distance, la base de données, l'algorithme ainsi que l'appartenance ethnique, l'âge ou le sexe de l'individu. Il en va de même pour la reconnaissance de la voix ou de la démarche et les autres systèmes biométriques. Le taux de fausses acceptations des systèmes hautement avancés est en diminution constante. Si un taux de 99 % de résultats exacts peut sembler satisfaisant de manière générale, il n'en présente pas moins un risque élevé si le résultat peut conduire à soupçonner un innocent. Même un taux d'erreur de 0,1 % peut avoir une incidence importante lorsqu'il s'applique à un grand nombre de personnes, par exemple dans les gares ferroviaires."
Comment les règles protègeront-elles les droits fondamentaux?,"Bien qu'il existe déjà une solide protection en faveur des droits fondamentaux et contre les discriminations au niveau de l'UE et des États membres, la complexité et l'opacité de certaines applications d'IA («boîtes noires») peuvent poser problème. Une approche de l'IA centrée sur l'humain implique de veiller à ce que les applications d'IA respectent la législation relative aux droits fondamentaux. En intégrant des exigences de responsabilité et de transparence dans le développement des systèmes d'IA à haut risque et en améliorant les capacités de contrôle de l'exécution de ces exigences, nous pouvons faire en sorte que l'obligation de conformité à la législation soit, dès le départ, inhérente à la conception de ces systèmes. En cas d'infraction, les autorités nationales auront accès, du fait de ces exigences, aux informations nécessaires pour enquêter et vérifier si l'IA a été utilisée dans le respect du droit de l'UE. En outre, le règlement sur l'IA soumet certains déployeurs de systèmes d'IA à haut risque à l'obligation de procéder à une analyse d'impact sur les droits fondamentaux."
Qu'est-ce qu'une analyse d'impact sur les droits fondamentaux? Qui doit procéder à une telle analyse d'impact et quand?,"Les fournisseurs de systèmes d'IA à haut risque sont tenus de réaliser une évaluation des risques et de concevoir leurs systèmes de façon à ce que les risques pour la santé, la sécurité et les droits fondamentaux soient réduits au maximum. Toutefois, certains risques pour les droits fondamentaux ne peuvent être pleinement identifiés que si le contexte de l'utilisation du système d'IA à haut risque est connu. Lorsque des systèmes d'IA à haut risque sont utilisés dans des domaines particulièrement sensibles présentant une éventuelle asymétrie de puissance, une plus grande prise en considération de ces risques est nécessaire. Par conséquent, les déployeurs qui sont des organismes de droit public ou des opérateurs privés prestataires de services publics ainsi que les opérateurs fournissant des systèmes d'IA à haut risque qui effectuent des évaluations de solvabilité ou des évaluations des tarifs et des risques en matière d'assurance-vie et d'assurance maladie, devront procéder à une analyse d'impact sur les droits fondamentaux et en notifier les résultats à l'autorité nationale. Dans la pratique, de nombreux déployeurs devront également réaliser une analyse d'impact relative à la protection des données. Afin d'éviter des chevauchements substantiels en pareils cas, l'analyse d'impact sur les droits fondamentaux devra être menée conjointement avec celle relative à la protection des données."
Comment le règlement sur l'IA traite-t-il le problème des biais sexistes ou raciaux dans le cadre de l'IA?,"Il est essentiel de souligner que les systèmes d'IA ne créent pas ni ne reproduisent de biais. Au contraire, bien conçus et utilisés, les systèmes d'IA peuvent contribuer à réduire les discriminations structurelles et les biais existants, et conduire ainsi à des décisions plus équitables et non discriminatoires (par exemple en matière de recrutement). Les nouvelles exigences contraignantes qui s'appliqueront à tous les systèmes d'IA à haut risque serviront cet objectif. Il faut que les systèmes d'IA soient techniquement robustes afin qu'ils soient adaptés à leur finalité et ne produisent pas de résultats biaisés, tels que de faux positifs ou négatifs, qui touchent de manière disproportionnée les groupes marginalisés, notamment des résultats biaisés fondés sur l'origine raciale ou ethnique, le sexe, l'âge et d'autres caractéristiques protégées. Il faudra également que les systèmes d'IA à haut risque soient entraînés et testés sur la base d'ensembles de données suffisamment représentatifs, afin de réduire au minimum le risque de biais injustes dans la conception même du modèle et de permettre un traitement adéquat de ces biais par des mesures de détection et de correction appropriées et par d'autres mesures d'atténuation. Ces systèmes d'IA devront aussi être traçables et auditables, d'où la nécessité de conserver une documentation appropriée, y compris les données utilisées pour entraîner l'algorithme, essentielles pour toute enquête a posteriori. Un système de vérification de la conformité avant et après mise sur le marché devra garantir une surveillance régulière de ces systèmes d'IA et un traitement rapide des risques potentiels."
Quand le règlement sur l'IA sera-t-il pleinement applicable?,"Le règlement sur l'IA s'appliquera deux ans après son entrée en vigueur, le 2 août 2026, à l'exception des dispositions particulières suivantes: les interdictions, définitions et dispositions relatives à la maîtrise de l'IA s'appliqueront six mois après l'entrée en vigueur, le 2 février 2025; les règles de gouvernance et les obligations en matière d'IA à usage général seront applicables 12 mois après l'entrée en vigueur, le 2 août 2025; les obligations applicables aux systèmes d'IA à haut risque qui sont classés comme étant à haut risque parce qu'ils sont intégrés dans des produits réglementés, énumérés à l'annexe II (liste d'actes législatifs d'harmonisation de l'Union), s'appliqueront 36 mois après l'entrée en vigueur, le 2 août 2027."
Comment fera-t-on respecter le règlement sur l'IA?,"Le règlement sur l'IA établit un système de gouvernance à deux niveaux, au sein duquel les autorités nationales sont chargées de superviser et de faire respecter les règles applicables aux systèmes d'IA, tandis que les modèles d'IA à usage général relèvent de la gouvernance de l'UE. Afin de garantir la cohérence et la coopération à l'échelle de l'UE, le Comité européen de l'intelligence artificielle («Comité IA») sera créé et sera constitué de représentants des États membres;il établira, en outre, des sous-groupes spécialisés pour les régulateurs nationaux et d'autres autorités compétentes. Le Bureau de l'IA, l'organe de mise en œuvre du règlement sur l'IA de la Commission, fournira des orientations stratégiques au Comité IA. En outre, le règlement sur l'IA institue deux organes consultatifs chargés de fournir une expertise: le groupe scientifique et le forum consultatif. Ces instances offriront des informations précieuses émanant de parties prenantes et de communautés scientifiques interdisciplinaires, qui éclaireront la prise de décision et favoriseront une approche équilibrée du développement de l'IA."
Pourquoi est-il nécessaire de créer un Comité européen de l'intelligence artificielle et quel sera son rôle?,"Le Comité européen de l'intelligence artificielle sera constitué de représentants de haut niveau des États membres et du Contrôleur européen de la protection des données. En tant que principale instance de conseil, le Comité IA fournira des orientations sur toutes les questions liées à la politique en matière d'IA, notamment en ce qui concerne la réglementation de l'IA, la politique d'innovation et d'excellence et la coopération internationale sur l'IA. Le rôle du Comité IA est crucial pour assurer la mise en œuvre aisée, efficace et harmonisée du règlement sur l'IA. Le Comité fera office de forum au sein duquel les régulateurs de l'IA, à savoir le Bureau de l'IA, les autorités nationales et le CEPD, pourront coordonner l'application cohérente du règlement sur l'IA."
Quelles seront les sanctions en cas d'infraction?,"Les États membres devront instaurer des sanctions effectives, proportionnées et dissuasives en cas de violation des règles applicables aux systèmes d'IA. Le règlement fixe des seuils qui devront être pris en considération: jusqu'à 35 millions d'EUR ou 7 % du chiffre d'affaires annuel mondial total réalisé au cours de l'exercice précédent (le montant le plus élevé étant retenu) en cas d'infractions correspondant à des pratiques interdites ou à un non-respect des exigences relatives aux données; jusqu'à 15 millions d'euros ou 3 % du chiffre d'affaires annuel mondial total réalisé au cours de l'exercice précédent en cas d'inobservation d'une des autres exigences ou obligations prévues par le règlement; jusqu'à 7,5 millions d'EUR ou 1,5 % du chiffre d'affaires annuel mondial total réalisé au cours de l'exercice précédent si des informations inexactes, incomplètes ou trompeuses ont été fournies aux organismes notifiés ou aux autorités nationales compétentes en réponse à une demande; pour chaque catégorie d'infraction, le seuil sera le plus faible des deux montants pour les PME et le plus élevé des deux pour les autres entreprises. La Commission peut également faire respecter les règles relatives aux fournisseurs de modèles d'IA à usage général en infligeant des amendes, en tenant compte du seuil suivant: jusqu'à 15 millions d'euros ou 3 % du chiffre d'affaires annuel mondial total réalisé au cours de l'exercice précédent en cas de non-respect d'une des autres obligations ou mesures exigées par la Commission en vertu du règlement. Parce que les institutions, agences et organes de l'Union sont censés montrer l'exemple, ils devront également se conformer aux règles et pourront encourir des sanctions. Le Contrôleur européen de la protection des données aura le pouvoir de leur infliger des amendes en cas de non-respect de ces règles."
Comment le code de bonne pratique de l'IA à usage général sera-t-il élaboré?,"L'élaboration de ce premier code obéit à un processus inclusif et transparent. Une séance plénière consacrée au code de bonne pratique sera organisée pour faciliter le processus itératif de rédaction; elle réunira tous les fournisseurs intéressés et éligibles de modèles d'IA à usage général, les fournisseurs en aval intégrant un modèle d'IA à usage général dans leur système d'IA, d'autres organisations sectorielles, d'autres organisations de parties prenantes telles que les organisations de la société civile ou les organisations de titulaires de droits, ainsi que le monde universitaire et d'autres experts indépendants. Le Bureau de l'IA a lancé un appel à manifestation d'intérêt pour la participation à l'élaboration du premier code de bonne pratique. Parallèlement à cet appel à manifestation d'intérêt, une consultation multipartite a été lancée afin de recueillir les avis et les contributions de toutes les parties intéressées concernant le premier code de bonne pratique. Les réponses et les observations constitueront la base de la première mouture de ce code de bonne pratique. Un large éventail de points de vue et une vaste expertise contribueront dès le départ à l'élaboration de ce code. La séance plénière sera structurée en quatre groupes de travail afin que puissent être menées des discussions ciblées sur des sujets spécifiques pertinents pour ce qui est de préciser les obligations qui incombent aux fournisseurs de modèles d'IA à usage général et à ceux de modèles d'IA à usage général présentant un risque systémique. Il sera loisible aux participants à la séance plénière de choisir le ou les groupes de travail auxquels ils souhaitent prendre part. Les réunions se tiendront exclusivement en ligne. Pour chacun des quatre groupes de travail de la séance plénière, le Bureau de l'IA nommera des présidents et, s'il y a lieu, des vice-présidents, choisis parmi les experts indépendants intéressés. Les présidents feront une synthèse des contributions et des commentaires des participants à la séance plénière en vue de rédiger par étapes le premier code de bonne pratique. En tant que principaux destinataires de ce code, les fournisseurs de modèles d'IA à usage général seront invités, en plus de leur participation à la séance plénière, à des ateliers spécialisés afin de contribuer à guider chaque cycle de rédaction itérative. Après neuf mois, la version définitive du premier code de bonne pratique sera présentée lors d'une séance plénière de clôture, prévue en avril, et sera publiée. Lors de cette séance plénière de clôture, les fournisseurs de modèles d'IA à usage général pourront indiquer s'ils entendent utiliser ce code."
"En quoi le code de bonne pratique pour les fournisseurs de modèles d'IA à usage général, s'il est approuvé, constituera-t-il un outil central pour assurer le respect des obligations?","À l'issue du processus d'élaboration du code de bonne pratique, le Bureau de l'IA et le Comité IA évalueront l'adéquation du code et rendront publique leur évaluation. La Commission pourra ensuite décider d'approuver un code de bonne pratique et de lui conférer une validité générale au sein de l'Union par voie d'actes d'exécution. Si, au moment où le règlement entre en application, le Bureau de l'IA estime que le code de bonne pratique n'est pas adéquat, la Commission pourra prévoir des règles communes pour la mise en œuvre des obligations pertinentes. Les fournisseurs de modèles d'IA à usage général pourront donc s'appuyer sur le code de bonne pratique pour démontrer qu'ils respectent les obligations énoncées dans le règlement sur l'IA. Conformément au règlement sur l'IA, le code de bonne pratique devrait inclure des objectifs, des mesures et, le cas échéant, des indicateurs de performance clés. Les fournisseurs qui adhèrent au code devraient régulièrement rendre compte au Bureau de l'IA de la mise en œuvre des mesures prises et de leurs résultats, y compris, le cas échéant, par rapport aux indicateurs de performance clés. Cela facilitera le contrôle du respect de la règlementation par le Bureau de l'IA, sur le fondement des pouvoirs conférés à la Commission par le règlement sur l'IA. Ces pouvoirs comprendront notamment la capacité de procéder à des évaluations de modèles d'IA à usage général, de demander aux fournisseurs de ces modèles qu'ils communiquent des informations et prennent des mesures, et d'appliquer des sanctions. Le Bureau de l'IA encouragera et facilitera, s'il y a lieu, le réexamen et l'adaptation du code afin qu'il soit tenu compte des progrès technologiques et de l'état de la technique. Dès lors qu'une norme harmonisée aura été publiée et jugée appropriée par le Bureau de l'IA au regard des obligations pertinentes, les fournisseurs de modèles d'IA devraient bénéficier de la présomption de conformité lorsqu'ils respectent une norme européenne harmonisée. En outre, les fournisseurs de modèles d'IA à usage général devraient être en mesure de démontrer la conformité en utilisant d'autres moyens adéquats en l'absence de codes de bonne pratique ou de normes harmonisées ou s'ils choisissent de ne pas s'appuyer sur ceux-ci. Le règlement sur l'IA contient-il des dispositions relatives à la protection de l'environnement et à la durabilité? L'objectif de la proposition de règlement sur l'IA est de parer aux risques pour la sécurité et les droits fondamentaux, y compris le droit fondamental à un niveau élevé de protection de l'environnement. L'environnement est également l'un des intérêts juridiques explicitement mentionnés et protégés. La Commission est invitée à demander aux organisations européennes de normalisation d'élaborer des livrables en matière de normalisation relatifs aux processus de déclaration et de documentation afin d'améliorer la performance des systèmes d'IA en matière de ressources, par exemple concernant la réduction de la consommation d'énergie et d'autres ressources par le système d'IA à haut risque tout au long de son cycle de vie, et relatifs au développement économe en énergie de modèles d'IA à usage général. De surcroît, la Commission est chargée de présenter, au plus tard deux ans après la date d'entrée en application du règlement et tous les quatre ans par la suite, un rapport sur l'examen de l'état d'avancement des travaux de normalisation relatifs au développement économe en énergie de modèles d'IA à usage général, et d'évaluer la nécessité de mesures ou actions supplémentaires, y compris de mesures ou actions contraignantes. En outre, les fournisseurs de modèles d'IA à usage général, qui sont entraînés avec de grandes quantités de données et ont donc tendance à consommer beaucoup d'énergie, seront tenus de divulguer leur consommation d'énergie. Quant aux modèles d'IA à usage général présentant des risques systémiques, il sera nécessaire d'en évaluer l'efficacité énergétique. La Commission est habilitée à élaborer une méthode de mesure appropriée et comparable pour ces obligations d'information."
En quoi les nouvelles règles soutiendront-elles l'innovation?,"Le cadre réglementaire peut favoriser l'adoption de l'IA de deux manières. D'une part, si la confiance des utilisateurs s'accroît, la demande d'IA des entreprises et des pouvoirs publics pour leur usage propre augmentera aussi. D'autre part, l'harmonisation des règles et le renforcement de la sécurité juridique permettront aux fournisseurs d'IA d'accéder à des marchés plus grands, en proposant des produits que les utilisateurs et les consommateurs apprécieront et achèteront. Les règles ne s'appliqueront que lorsque cela sera strictement nécessaire et d'une façon qui imposera le moins de contraintes possible aux opérateurs économiques, grâce à une structure de gouvernance légère. Le règlement sur l'IA permet en outre la création de bacs à sable réglementaires et la réalisation d'essais en conditions réelles, qui offrent un environnement contrôlé pour tester des technologies innovantes pendant une période limitée, favorisant ainsi l'innovation par les entreprises, les PME et les jeunes pousses dans le respect du règlement sur l'IA. Ces initiatives, combinées à d'autres mesures telles que les nouveaux réseaux de centres d'excellence en matière d'IA et le partenariat public-privé sur l'intelligence artificielle, les données et la robotique, ainsi que l'accès aux pôles européens d'innovation numérique et aux installations d'essai et d'expérimentation, contribueront à créer les conditions-cadres propices au développement et au déploiement de l'IA par les entreprises. Les essais en conditions réelles des systèmes d'IA à haut risque peuvent être menés pendant une durée maximale de six mois (qui peut être prolongée de six mois supplémentaires). Avant les essais, il convient d'établir un plan et de le soumettre à l'autorité de surveillance du marché, qui doit approuver ce plan ainsi que les conditions d'essai particulières, ceux-ci étant par défaut approuvés de manière tacite si aucune réponse n'a été donnée dans un délai de 30 jours. Les essais peuvent faire l'objet d'inspections inopinées de la part de l'autorité. Les essais en conditions réelles ne peuvent être réalisés que moyennant des garanties particulières: par exemple, les utilisateurs des systèmes testés en conditions réelles doivent donner leur consentement éclairé, les essais ne doivent pas avoir d'effet négatif sur ces utilisateurs, les résultats doivent être réversibles ou doivent pouvoir être ignorés, et les données des utilisateurs doivent être supprimées après l'achèvement de l'essai. Une protection particulière sera accordée aux groupes vulnérables, c'est-à-dire aux personnes âgées ou porteuses d'un handicap physique ou mental."
Quel est le rôle du pacte sur l'IA dans la mise en œuvre du règlement sur l'IA?,"Lancé par le commissaire Breton en mai 2023, le pacte sur l'IA vise à renforcer le dialogue entre le Bureau de l'IA et les organisations d'IA (pilier I) et à encourager l'engagement volontaire de l'industrie à commencer à mettre en œuvre les exigences du règlement sur l'IA avant l'échéance légale (pilier II). En particulier, dans le cadre du pilier I, les participants contribueront à la création d'une communauté collaborative, en partageant leurs expériences et leurs connaissances. Cela se traduira notamment par l'organisation d'ateliers par le Bureau de l'IA, qui permettront aux participants de mieux comprendre le règlement sur l'IA, de prendre la mesure de leurs responsabilités et de s'informer sur les modalités de préparation à sa mise en œuvre. Le Bureau de l'IA pourra, quant à lui, recueillir des informations sur les meilleures pratiques et les défis auxquels sont confrontés les participants. Dans le cadre du pilier II, les organisations seront encouragées à divulguer de manière proactive les processus et pratiques qu'elles mettent en œuvre pour anticiper la mise en conformité, au moyen d'engagements volontaires. Ces engagements, conçus comme des «déclarations d'engagement», comprendront des actions (prévues ou en cours) visant à satisfaire à certaines exigences du règlement sur l'IA. La majorité des dispositions du règlement sur l'IA (par exemple, certaines exigences relatives aux systèmes d'IA à haut risque) s'appliqueront à la fin d'une période transitoire (c'est-à-dire la période comprise entre l'entrée en vigueur et la date d'applicabilité). Dans ce contexte et dans le cadre du pacte sur l'IA, le Bureau de l'IA invite toutes les organisations à anticiper et à mettre en œuvre de manière proactive certaines dispositions essentielles du règlement sur l'IA, dans le but d'atténuer les risques pour la santé, la sécurité et les droits fondamentaux dès que possible. À la suite d'un appel lancé en novembre 2023, plus de 700 organisations ont déjà manifesté leur intérêt à adhérer à l'initiative du pacte sur l'IA. Une première séance d'information qui a réuni 300 participants s'est tenue en ligne le 6 mai. La signature officielle des engagements volontaires est prévue pour l'automne 2024. Un atelier consacré au pacte sur l'IA aura lieu au cours de la première semaine de septembre."
Quelle est la dimension internationale de l'approche de l'UE?,"Les conséquences et les défis de l'IA dépassent les frontières; c'est pourquoi la coopération internationale revêt une grande importance. Le Bureau de l'IA est responsable de la mobilisation internationale de l'Union européenne dans le domaine de l'IA, sur le fondement du règlement sur l'IA et du plan coordonné en la matière. L'UE cherche à promouvoir la gestion responsable et la bonne gouvernance de l'IA en collaboration avec les partenaires internationaux et conformément au système multilatéral fondé sur des règles et aux valeurs qu'elle défend. L'UE s'emploie, de manière bilatérale et multilatérale, à promouvoir une IA digne de confiance, centrée sur l'humain et éthique. Aussi participe-t-elle aux forums multilatéraux au sein desquels l'IA est examinée — notamment le G7, le G20, l'OCDE, le Conseil de l'Europe, le partenariat mondial sur l'IA et les Nations unies — et entretient-elle des liens bilatéraux étroits avec, par exemple, le Canada, les États-Unis, l'Inde, le Japon, la Corée du Sud, Singapour et la région d'Amérique latine et des Caraïbes. * Mis à jour le 1.8.2024 QANDA/21/1683 Personnes de contact pour la presse: Thomas Regnier (+32 2 29 9 1099) Patricia Poropat (+32 2 298 04 85) Renseignements au public: Europe Direct par téléphone au 00 800 67 89 10 11 ou par courriel"
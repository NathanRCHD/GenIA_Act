{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2cc681",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Evaluate an OpenWebUI-powered compliance chatbot against a Q&A gold set.\n",
    "\n",
    "What it does:\n",
    "1) Builds a CSV with 2 columns: question, answer (gold), from the provided PDF.\n",
    "2) Sends each question to OpenWebUI Chat Completions endpoint and stores model answers.\n",
    "3) Uses an evaluation model (via the same OpenWebUI endpoint, or any OpenAI-compatible endpoint)\n",
    "   to score:\n",
    "   - fidelity_score (0-5): faithfulness to the gold answer (no contradictions, covers key points)\n",
    "   - quality_score  (0-5): clarity, completeness, usefulness (structure, actionable, concise)\n",
    "4) Writes a results CSV and a JSONL log for auditability.\n",
    "\n",
    "Requirements:\n",
    "  pip install pdfplumber pandas requests python-dotenv\n",
    "\n",
    "Environment variables (recommended via .env):\n",
    "  # OpenWebUI target (the model you're evaluating)\n",
    "  OWUI_BASE_URL=http://localhost:3000\n",
    "  OWUI_API_KEY=            # optional depending on your setup\n",
    "  OWUI_MODEL=llama3.1:8b   # any model visible to OpenWebUI\n",
    "  OWUI_ENDPOINT=/api/chat/completions  # default; could also be /v1/chat/completions on newer setups\n",
    "\n",
    "  # Evaluator (can be the same OpenWebUI or a different OpenAI-compatible server)\n",
    "  EVAL_BASE_URL=http://localhost:3000\n",
    "  EVAL_API_KEY=\n",
    "  EVAL_MODEL=qwen2.5:14b-instruct\n",
    "  EVAL_ENDPOINT=/api/chat/completions\n",
    "\n",
    "Usage:\n",
    "  python eval_chatbot.py \\\n",
    "    --pdf \"Intelligence_artificielle_-_Questions_et_r_ponses_.pdf\" \\\n",
    "    --out results.csv\n",
    "\n",
    "Notes:\n",
    "- This script assumes the PDF is a Commission \"Questions et réponses\" format, where questions are headings.\n",
    "- The extraction heuristic may need minor tuning if your PDF formatting differs.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import requests\n",
    "\n",
    "INTERROG_STARTS = (\n",
    "    \"Pourquoi\", \"À qui\", \"Quelles\", \"Quels\", \"Comment\", \"Quand\",\n",
    "    \"En quoi\", \"Quel\", \"Qu'est-ce\", \"Qu’\", \"Qu'\", \"À quoi\",\n",
    ")\n",
    "\n",
    "def extract_qa_from_pdf(pdf_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Extract (question, answer) pairs from the PDF using simple heading heuristics.\"\"\"\n",
    "    text_pages: List[str] = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text_pages.append(page.extract_text() or \"\")\n",
    "\n",
    "    all_lines: List[str] = []\n",
    "    for pg in text_pages:\n",
    "        all_lines += pg.splitlines()\n",
    "    all_lines = [l.strip() for l in all_lines if l and l.strip()]\n",
    "\n",
    "    pairs: List[Dict[str, str]] = []\n",
    "    i = 0\n",
    "    while i < len(all_lines):\n",
    "        line = all_lines[i]\n",
    "        if line.startswith(INTERROG_STARTS):\n",
    "            q_lines = [line]\n",
    "            while not q_lines[-1].endswith(\"?\") and i + 1 < len(all_lines):\n",
    "                i += 1\n",
    "                q_lines.append(all_lines[i].strip())\n",
    "                if len(\" \".join(q_lines)) > 350:\n",
    "                    break\n",
    "\n",
    "            question = \" \".join(q_lines).replace(\"  \", \" \").strip()\n",
    "\n",
    "            ans_lines: List[str] = []\n",
    "            i += 1\n",
    "            while i < len(all_lines):\n",
    "                nxt = all_lines[i]\n",
    "                if nxt.startswith(INTERROG_STARTS):\n",
    "                    i -= 1\n",
    "                    break\n",
    "                ans_lines.append(nxt)\n",
    "                i += 1\n",
    "\n",
    "            answer = \" \".join(ans_lines).replace(\"  \", \" \").strip()\n",
    "            pairs.append({\"question\": question, \"answer\": answer})\n",
    "        i += 1\n",
    "\n",
    "    if not pairs:\n",
    "        raise RuntimeError(\"No Q&A pairs extracted. Check PDF format or tweak INTERROG_STARTS.\")\n",
    "    return pd.DataFrame(pairs)\n",
    "\n",
    "\n",
    "def _headers(api_key: str) -> Dict[str, str]:\n",
    "    h = {\"Content-Type\": \"application/json\"}\n",
    "    if api_key:\n",
    "        h[\"Authorization\"] = f\"Bearer {api_key}\"\n",
    "    return h\n",
    "\n",
    "\n",
    "def call_chat_completions(\n",
    "    base_url: str,\n",
    "    endpoint: str,\n",
    "    api_key: str,\n",
    "    model: str,\n",
    "    messages: List[Dict[str, str]],\n",
    "    temperature: float = 0.2,\n",
    "    timeout_s: int = 120,\n",
    ") -> Dict:\n",
    "    url = base_url.rstrip(\"/\") + endpoint\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"stream\": False,\n",
    "    }\n",
    "    r = requests.post(url, headers=_headers(api_key), json=payload, timeout=timeout_s)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "\n",
    "def extract_text_from_response(resp: Dict) -> str:\n",
    "    \"\"\"\n",
    "    OpenAI-style response typically:\n",
    "      resp[\"choices\"][0][\"message\"][\"content\"]\n",
    "    Some servers may return variants; this handles common cases.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return (resp.get(\"choices\", [{}])[0].get(\"message\", {}) or {}).get(\"content\", \"\") or \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "EVAL_SYSTEM = \"\"\"You are a strict evaluator for an EU AI compliance chatbot.\n",
    "You will be given:\n",
    "- QUESTION\n",
    "- GOLD_ANSWER (reference)\n",
    "- MODEL_ANSWER (candidate)\n",
    "\n",
    "Return ONLY valid JSON with:\n",
    "{\n",
    "  \"fidelity_score\": <0-5 integer>,\n",
    "  \"quality_score\": <0-5 integer>,\n",
    "  \"fidelity_rationale\": \"<brief>\",\n",
    "  \"quality_rationale\": \"<brief>\"\n",
    "}\n",
    "\n",
    "Scoring:\n",
    "- fidelity_score:\n",
    "  5 = fully aligned with gold; no contradictions; covers the key obligations/definitions.\n",
    "  3 = mostly aligned; minor omissions or slight overreach without contradiction.\n",
    "  1 = significant mismatch or misleading.\n",
    "  0 = unrelated or clearly wrong.\n",
    "\n",
    "- quality_score:\n",
    "  5 = clear, well-structured, directly answers, actionable, appropriately cautious.\n",
    "  3 = understandable but could be clearer/structured; some verbosity/ambiguity.\n",
    "  1 = confusing, poorly structured, or unhelpful.\n",
    "  0 = unusable.\n",
    "\n",
    "Rules:\n",
    "- Do not invent facts. Evaluate only based on GOLD_ANSWER.\n",
    "- Penalize MODEL_ANSWER if it adds claims not supported by GOLD_ANSWER (hallucination risk).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def safe_parse_json(text: str) -> Optional[Dict]:\n",
    "    # Try direct parse; if the model wrapped it, extract the first JSON object.\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        m = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group(0))\n",
    "            except Exception:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--pdf\", required=True, help=\"Path to the Q&A PDF.\")\n",
    "    ap.add_argument(\"--out\", default=\"results.csv\", help=\"Output CSV path.\")\n",
    "    ap.add_argument(\"--jsonl\", default=\"results.jsonl\", help=\"Audit log JSONL path.\")\n",
    "    ap.add_argument(\"--sleep\", type=float, default=0.2, help=\"Sleep seconds between calls.\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    # Load config\n",
    "    owui_base = os.getenv(\"OWUI_BASE_URL\", \"http://localhost:3000\")\n",
    "    owui_ep = os.getenv(\"OWUI_ENDPOINT\", \"/api/chat/completions\")\n",
    "    owui_key = os.getenv(\"OWUI_API_KEY\", \"\")\n",
    "    owui_model = os.getenv(\"OWUI_MODEL\", \"llama3.1:8b\")\n",
    "\n",
    "    eval_base = os.getenv(\"EVAL_BASE_URL\", owui_base)\n",
    "    eval_ep = os.getenv(\"EVAL_ENDPOINT\", owui_ep)\n",
    "    eval_key = os.getenv(\"EVAL_API_KEY\", owui_key)\n",
    "    eval_model = os.getenv(\"EVAL_MODEL\", owui_model)\n",
    "\n",
    "    df = extract_qa_from_pdf(args.pdf)\n",
    "    df[\"model_answer\"] = \"\"\n",
    "    df[\"fidelity_score\"] = None\n",
    "    df[\"quality_score\"] = None\n",
    "    df[\"fidelity_rationale\"] = \"\"\n",
    "    df[\"quality_rationale\"] = \"\"\n",
    "\n",
    "    os.makedirs(os.path.dirname(args.out) or \".\", exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(args.jsonl) or \".\", exist_ok=True)\n",
    "\n",
    "    with open(args.jsonl, \"w\", encoding=\"utf-8\") as fjsonl:\n",
    "        for idx, row in df.iterrows():\n",
    "            q = row[\"question\"]\n",
    "            gold = row[\"answer\"]\n",
    "\n",
    "            # 1) Call your chatbot (OpenWebUI)\n",
    "            resp = call_chat_completions(\n",
    "                base_url=owui_base,\n",
    "                endpoint=owui_ep,\n",
    "                api_key=owui_key,\n",
    "                model=owui_model,\n",
    "                messages=[{\"role\": \"user\", \"content\": q}],\n",
    "                temperature=0.2,\n",
    "            )\n",
    "            model_answer = extract_text_from_response(resp).strip()\n",
    "            df.at[idx, \"model_answer\"] = model_answer\n",
    "\n",
    "            time.sleep(args.sleep)\n",
    "\n",
    "            # 2) Evaluate with an evaluator model\n",
    "            eval_prompt = f\"\"\"QUESTION:\n",
    "{q}\n",
    "\n",
    "GOLD_ANSWER:\n",
    "{gold}\n",
    "\n",
    "MODEL_ANSWER:\n",
    "{model_answer}\n",
    "\"\"\"\n",
    "            eresp = call_chat_completions(\n",
    "                base_url=eval_base,\n",
    "                endpoint=eval_ep,\n",
    "                api_key=eval_key,\n",
    "                model=eval_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": EVAL_SYSTEM},\n",
    "                    {\"role\": \"user\", \"content\": eval_prompt},\n",
    "                ],\n",
    "                temperature=0.0,\n",
    "            )\n",
    "            etext = extract_text_from_response(eresp).strip()\n",
    "            ej = safe_parse_json(etext) or {}\n",
    "\n",
    "            # Defensive parsing\n",
    "            fidelity = ej.get(\"fidelity_score\", None)\n",
    "            quality = ej.get(\"quality_score\", None)\n",
    "\n",
    "            # Coerce to int in 0..5 if possible\n",
    "            def _coerce(v):\n",
    "                try:\n",
    "                    vi = int(v)\n",
    "                    return max(0, min(5, vi))\n",
    "                except Exception:\n",
    "                    return None\n",
    "\n",
    "            fidelity = _coerce(fidelity)\n",
    "            quality = _coerce(quality)\n",
    "\n",
    "            df.at[idx, \"fidelity_score\"] = fidelity\n",
    "            df.at[idx, \"quality_score\"] = quality\n",
    "            df.at[idx, \"fidelity_rationale\"] = (ej.get(\"fidelity_rationale\") or \"\").strip()\n",
    "            df.at[idx, \"quality_rationale\"] = (ej.get(\"quality_rationale\") or \"\").strip()\n",
    "\n",
    "            # Audit log line\n",
    "            fjsonl.write(json.dumps({\n",
    "                \"i\": int(idx),\n",
    "                \"question\": q,\n",
    "                \"gold_answer\": gold,\n",
    "                \"model_answer\": model_answer,\n",
    "                \"model_call\": {\"base_url\": owui_base, \"endpoint\": owui_ep, \"model\": owui_model},\n",
    "                \"eval_call\": {\"base_url\": eval_base, \"endpoint\": eval_ep, \"model\": eval_model},\n",
    "                \"eval_raw_text\": etext,\n",
    "                \"eval_parsed\": {\n",
    "                    \"fidelity_score\": fidelity,\n",
    "                    \"quality_score\": quality,\n",
    "                    \"fidelity_rationale\": df.at[idx, \"fidelity_rationale\"],\n",
    "                    \"quality_rationale\": df.at[idx, \"quality_rationale\"],\n",
    "                }\n",
    "            }, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            time.sleep(args.sleep)\n",
    "\n",
    "    # Aggregate metrics\n",
    "    fidelity_mean = df[\"fidelity_score\"].dropna().mean()\n",
    "    quality_mean = df[\"quality_score\"].dropna().mean()\n",
    "\n",
    "    # Add summary row (optional)\n",
    "    summary = {\n",
    "        \"question\": \"__SUMMARY__\",\n",
    "        \"answer\": \"\",\n",
    "        \"model_answer\": \"\",\n",
    "        \"fidelity_score\": fidelity_mean,\n",
    "        \"quality_score\": quality_mean,\n",
    "        \"fidelity_rationale\": \"mean over evaluated rows\",\n",
    "        \"quality_rationale\": \"mean over evaluated rows\",\n",
    "    }\n",
    "    df_out = pd.concat([df, pd.DataFrame([summary])], ignore_index=True)\n",
    "\n",
    "    df_out.to_csv(args.out, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Wrote: {args.out}\")\n",
    "    print(f\"Wrote: {args.jsonl}\")\n",
    "    print(f\"Mean fidelity_score: {fidelity_mean:.2f} / 5\")\n",
    "    print(f\"Mean quality_score : {quality_mean:.2f} / 5\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

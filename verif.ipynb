{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa2cc681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --pdf PDF [--out OUT] [--jsonl JSONL]\n",
      "                             [--sleep SLEEP]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --pdf\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\romeo\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Evaluate an OpenWebUI-powered compliance chatbot against a Q&A gold set.\n",
    "\n",
    "What it does:\n",
    "1) Builds a CSV with 2 columns: question, answer (gold), from the provided PDF.\n",
    "2) Sends each question to OpenWebUI Chat Completions endpoint and stores model answers.\n",
    "3) Uses an evaluation model (via the same OpenWebUI endpoint, or any OpenAI-compatible endpoint)\n",
    "   to score:\n",
    "   - fidelity_score (0-5): faithfulness to the gold answer (no contradictions, covers key points)\n",
    "   - quality_score  (0-5): clarity, completeness, usefulness (structure, actionable, concise)\n",
    "4) Writes a results CSV and a JSONL log for auditability.\n",
    "\n",
    "Requirements:\n",
    "  pip install pdfplumber pandas requests python-dotenv\n",
    "\n",
    "Environment variables (recommended via .env):\n",
    "  # OpenWebUI target (the model you're evaluating)\n",
    "  OWUI_BASE_URL=http://localhost:3000\n",
    "  OWUI_API_KEY=            # optional depending on your setup\n",
    "  OWUI_MODEL=llama3.1:8b   # any model visible to OpenWebUI\n",
    "  OWUI_ENDPOINT=/api/chat/completions  # default; could also be /v1/chat/completions on newer setups\n",
    "\n",
    "  # Evaluator (can be the same OpenWebUI or a different OpenAI-compatible server)\n",
    "  EVAL_BASE_URL=http://localhost:3000\n",
    "  EVAL_API_KEY=\n",
    "  EVAL_MODEL=qwen2.5:14b-instruct\n",
    "  EVAL_ENDPOINT=/api/chat/completions\n",
    "\n",
    "Usage:\n",
    "  python eval_chatbot.py \\\n",
    "    --pdf \"Intelligence_artificielle_-_Questions_et_r_ponses_.pdf\" \\\n",
    "    --out results.csv\n",
    "\n",
    "Notes:\n",
    "- This script assumes the PDF is a Commission \"Questions et réponses\" format, where questions are headings.\n",
    "- The extraction heuristic may need minor tuning if your PDF formatting differs.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import requests\n",
    "\n",
    "INTERROG_STARTS = (\n",
    "    \"Pourquoi\", \"À qui\", \"Quelles\", \"Quels\", \"Comment\", \"Quand\",\n",
    "    \"En quoi\", \"Quel\", \"Qu'est-ce\", \"Qu’\", \"Qu'\", \"À quoi\",\n",
    ")\n",
    "\n",
    "def extract_qa_from_pdf(pdf_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Extract (question, answer) pairs from the PDF using simple heading heuristics.\"\"\"\n",
    "    text_pages: List[str] = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text_pages.append(page.extract_text() or \"\")\n",
    "\n",
    "    all_lines: List[str] = []\n",
    "    for pg in text_pages:\n",
    "        all_lines += pg.splitlines()\n",
    "    all_lines = [l.strip() for l in all_lines if l and l.strip()]\n",
    "\n",
    "    pairs: List[Dict[str, str]] = []\n",
    "    i = 0\n",
    "    while i < len(all_lines):\n",
    "        line = all_lines[i]\n",
    "        if line.startswith(INTERROG_STARTS):\n",
    "            q_lines = [line]\n",
    "            while not q_lines[-1].endswith(\"?\") and i + 1 < len(all_lines):\n",
    "                i += 1\n",
    "                q_lines.append(all_lines[i].strip())\n",
    "                if len(\" \".join(q_lines)) > 350:\n",
    "                    break\n",
    "\n",
    "            question = \" \".join(q_lines).replace(\"  \", \" \").strip()\n",
    "\n",
    "            ans_lines: List[str] = []\n",
    "            i += 1\n",
    "            while i < len(all_lines):\n",
    "                nxt = all_lines[i]\n",
    "                if nxt.startswith(INTERROG_STARTS):\n",
    "                    i -= 1\n",
    "                    break\n",
    "                ans_lines.append(nxt)\n",
    "                i += 1\n",
    "\n",
    "            answer = \" \".join(ans_lines).replace(\"  \", \" \").strip()\n",
    "            pairs.append({\"question\": question, \"answer\": answer})\n",
    "        i += 1\n",
    "\n",
    "    if not pairs:\n",
    "        raise RuntimeError(\"No Q&A pairs extracted. Check PDF format or tweak INTERROG_STARTS.\")\n",
    "    return pd.DataFrame(pairs)\n",
    "\n",
    "\n",
    "def _headers(api_key: str) -> Dict[str, str]:\n",
    "    h = {\"Content-Type\": \"application/json\"}\n",
    "    if api_key:\n",
    "        h[\"Authorization\"] = f\"Bearer {api_key}\"\n",
    "    return h\n",
    "\n",
    "\n",
    "def call_chat_completions(\n",
    "    base_url: str,\n",
    "    endpoint: str,\n",
    "    api_key: str,\n",
    "    model: str,\n",
    "    messages: List[Dict[str, str]],\n",
    "    temperature: float = 0.2,\n",
    "    timeout_s: int = 120,\n",
    ") -> Dict:\n",
    "    url = base_url.rstrip(\"/\") + endpoint\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"stream\": False,\n",
    "    }\n",
    "    r = requests.post(url, headers=_headers(api_key), json=payload, timeout=timeout_s)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "\n",
    "def extract_text_from_response(resp: Dict) -> str:\n",
    "    \"\"\"\n",
    "    OpenAI-style response typically:\n",
    "      resp[\"choices\"][0][\"message\"][\"content\"]\n",
    "    Some servers may return variants; this handles common cases.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return (resp.get(\"choices\", [{}])[0].get(\"message\", {}) or {}).get(\"content\", \"\") or \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "EVAL_SYSTEM = \"\"\"You are a strict evaluator for an EU AI compliance chatbot.\n",
    "You will be given:\n",
    "- QUESTION\n",
    "- GOLD_ANSWER (reference)\n",
    "- MODEL_ANSWER (candidate)\n",
    "\n",
    "Return ONLY valid JSON with:\n",
    "{\n",
    "  \"fidelity_score\": <0-5 integer>,\n",
    "  \"quality_score\": <0-5 integer>,\n",
    "  \"fidelity_rationale\": \"<brief>\",\n",
    "  \"quality_rationale\": \"<brief>\"\n",
    "}\n",
    "\n",
    "Scoring:\n",
    "- fidelity_score:\n",
    "  5 = fully aligned with gold; no contradictions; covers the key obligations/definitions.\n",
    "  3 = mostly aligned; minor omissions or slight overreach without contradiction.\n",
    "  1 = significant mismatch or misleading.\n",
    "  0 = unrelated or clearly wrong.\n",
    "\n",
    "- quality_score:\n",
    "  5 = clear, well-structured, directly answers, actionable, appropriately cautious.\n",
    "  3 = understandable but could be clearer/structured; some verbosity/ambiguity.\n",
    "  1 = confusing, poorly structured, or unhelpful.\n",
    "  0 = unusable.\n",
    "\n",
    "Rules:\n",
    "- Do not invent facts. Evaluate only based on GOLD_ANSWER.\n",
    "- Penalize MODEL_ANSWER if it adds claims not supported by GOLD_ANSWER (hallucination risk).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def safe_parse_json(text: str) -> Optional[Dict]:\n",
    "    # Try direct parse; if the model wrapped it, extract the first JSON object.\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        m = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group(0))\n",
    "            except Exception:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--pdf\", required=True, help=\"Path to the Q&A PDF.\")\n",
    "    ap.add_argument(\"--out\", default=\"results.csv\", help=\"Output CSV path.\")\n",
    "    ap.add_argument(\"--jsonl\", default=\"results.jsonl\", help=\"Audit log JSONL path.\")\n",
    "    ap.add_argument(\"--sleep\", type=float, default=0.2, help=\"Sleep seconds between calls.\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    # Load config\n",
    "    owui_base = os.getenv(\"OWUI_BASE_URL\", \"https://k2vm-74.mde.epf.fr/api/chat/completions\")\n",
    "    owui_ep = os.getenv(\"OWUI_ENDPOINT\", \"/api/chat/completions\")\n",
    "    owui_key = os.getenv(\"OWUI_API_KEY\", \"\")\n",
    "    owui_model = os.getenv(\"OWUI_MODEL\", \"n8n\")\n",
    "\n",
    "    eval_base = os.getenv(\"EVAL_BASE_URL\", owui_base)\n",
    "    eval_ep = os.getenv(\"EVAL_ENDPOINT\", owui_ep)\n",
    "    eval_key = os.getenv(\"EVAL_API_KEY\", owui_key)\n",
    "    eval_model = os.getenv(\"EVAL_MODEL\", owui_model)\n",
    "\n",
    "    df = extract_qa_from_pdf(args.pdf)\n",
    "    df[\"model_answer\"] = \"\"\n",
    "    df[\"fidelity_score\"] = None\n",
    "    df[\"quality_score\"] = None\n",
    "    df[\"fidelity_rationale\"] = \"\"\n",
    "    df[\"quality_rationale\"] = \"\"\n",
    "\n",
    "    os.makedirs(os.path.dirname(args.out) or \".\", exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(args.jsonl) or \".\", exist_ok=True)\n",
    "\n",
    "    with open(args.jsonl, \"w\", encoding=\"utf-8\") as fjsonl:\n",
    "        for idx, row in df.iterrows():\n",
    "            q = row[\"question\"]\n",
    "            gold = row[\"answer\"]\n",
    "\n",
    "            # 1) Call your chatbot (OpenWebUI)\n",
    "            resp = call_chat_completions(\n",
    "                base_url=owui_base,\n",
    "                endpoint=owui_ep,\n",
    "                api_key=owui_key,\n",
    "                model=owui_model,\n",
    "                messages=[{\"role\": \"user\", \"content\": q}],\n",
    "                temperature=0.2,\n",
    "            )\n",
    "            model_answer = extract_text_from_response(resp).strip()\n",
    "            df.at[idx, \"model_answer\"] = model_answer\n",
    "\n",
    "            time.sleep(args.sleep)\n",
    "\n",
    "            # 2) Evaluate with an evaluator model\n",
    "            eval_prompt = f\"\"\"QUESTION:\n",
    "{q}\n",
    "\n",
    "GOLD_ANSWER:\n",
    "{gold}\n",
    "\n",
    "MODEL_ANSWER:\n",
    "{model_answer}\n",
    "\"\"\"\n",
    "            eresp = call_chat_completions(\n",
    "                base_url=eval_base,\n",
    "                endpoint=eval_ep,\n",
    "                api_key=eval_key,\n",
    "                model=eval_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": EVAL_SYSTEM},\n",
    "                    {\"role\": \"user\", \"content\": eval_prompt},\n",
    "                ],\n",
    "                temperature=0.0,\n",
    "            )\n",
    "            etext = extract_text_from_response(eresp).strip()\n",
    "            ej = safe_parse_json(etext) or {}\n",
    "\n",
    "            # Defensive parsing\n",
    "            fidelity = ej.get(\"fidelity_score\", None)\n",
    "            quality = ej.get(\"quality_score\", None)\n",
    "\n",
    "            # Coerce to int in 0..5 if possible\n",
    "            def _coerce(v):\n",
    "                try:\n",
    "                    vi = int(v)\n",
    "                    return max(0, min(5, vi))\n",
    "                except Exception:\n",
    "                    return None\n",
    "\n",
    "            fidelity = _coerce(fidelity)\n",
    "            quality = _coerce(quality)\n",
    "\n",
    "            df.at[idx, \"fidelity_score\"] = fidelity\n",
    "            df.at[idx, \"quality_score\"] = quality\n",
    "            df.at[idx, \"fidelity_rationale\"] = (ej.get(\"fidelity_rationale\") or \"\").strip()\n",
    "            df.at[idx, \"quality_rationale\"] = (ej.get(\"quality_rationale\") or \"\").strip()\n",
    "\n",
    "            # Audit log line\n",
    "            fjsonl.write(json.dumps({\n",
    "                \"i\": int(idx),\n",
    "                \"question\": q,\n",
    "                \"gold_answer\": gold,\n",
    "                \"model_answer\": model_answer,\n",
    "                \"model_call\": {\"base_url\": owui_base, \"endpoint\": owui_ep, \"model\": owui_model},\n",
    "                \"eval_call\": {\"base_url\": eval_base, \"endpoint\": eval_ep, \"model\": eval_model},\n",
    "                \"eval_raw_text\": etext,\n",
    "                \"eval_parsed\": {\n",
    "                    \"fidelity_score\": fidelity,\n",
    "                    \"quality_score\": quality,\n",
    "                    \"fidelity_rationale\": df.at[idx, \"fidelity_rationale\"],\n",
    "                    \"quality_rationale\": df.at[idx, \"quality_rationale\"],\n",
    "                }\n",
    "            }, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            time.sleep(args.sleep)\n",
    "\n",
    "    # Aggregate metrics\n",
    "    fidelity_mean = df[\"fidelity_score\"].dropna().mean()\n",
    "    quality_mean = df[\"quality_score\"].dropna().mean()\n",
    "\n",
    "    # Add summary row (optional)\n",
    "    summary = {\n",
    "        \"question\": \"__SUMMARY__\",\n",
    "        \"answer\": \"\",\n",
    "        \"model_answer\": \"\",\n",
    "        \"fidelity_score\": fidelity_mean,\n",
    "        \"quality_score\": quality_mean,\n",
    "        \"fidelity_rationale\": \"mean over evaluated rows\",\n",
    "        \"quality_rationale\": \"mean over evaluated rows\",\n",
    "    }\n",
    "    df_out = pd.concat([df, pd.DataFrame([summary])], ignore_index=True)\n",
    "\n",
    "    df_out.to_csv(args.out, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Wrote: {args.out}\")\n",
    "    print(f\"Wrote: {args.jsonl}\")\n",
    "    print(f\"Mean fidelity_score: {fidelity_mean:.2f} / 5\")\n",
    "    print(f\"Mean quality_score : {quality_mean:.2f} / 5\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7f7c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, Any, Tuple, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 0) CONFIG\n",
    "# =========================\n",
    "\n",
    "# OpenWebUI base URL (ex: http://localhost:3000 or http://openwebui:8080)\n",
    "OPENWEBUI_BASE_URL = os.getenv(\"OPENWEBUI_BASE_URL\", \"http://localhost:3000\")\n",
    "\n",
    "# OpenWebUI often supports OpenAI-compatible routes under /v1\n",
    "CHAT_COMPLETIONS_PATH = os.getenv(\"OPENWEBUI_CHAT_PATH\", \"/v1/chat/completions\")\n",
    "\n",
    "# Model used to generate answers (your compliance chatbot model)\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"ollama:llama3.1\")  # example\n",
    "\n",
    "# Validation/judge model (can be different from MODEL_NAME)\n",
    "JUDGE_MODEL_NAME = os.getenv(\"JUDGE_MODEL_NAME\", \"ollama:qwen2.5\")  # example\n",
    "\n",
    "# If your OpenWebUI is protected, provide bearer token\n",
    "OPENWEBUI_API_KEY = os.getenv(\"OPENWEBUI_API_KEY\", \"\")  # optional\n",
    "\n",
    "# Input / Output\n",
    "INPUT_CSV = os.getenv(\"INPUT_CSV\", \"eu_ai_act_qna_dataset.csv\")\n",
    "OUTPUT_CSV = os.getenv(\"OUTPUT_CSV\", \"eu_ai_act_qna_scored.csv\")\n",
    "\n",
    "# Throttling / reliability\n",
    "REQUEST_TIMEOUT_S = int(os.getenv(\"REQUEST_TIMEOUT_S\", \"120\"))\n",
    "SLEEP_BETWEEN_CALLS_S = float(os.getenv(\"SLEEP_BETWEEN_CALLS_S\", \"0.5\"))\n",
    "MAX_RETRIES = int(os.getenv(\"MAX_RETRIES\", \"3\"))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) LOW-LEVEL API CALL\n",
    "# =========================\n",
    "\n",
    "def _headers() -> Dict[str, str]:\n",
    "    h = {\"Content-Type\": \"application/json\"}\n",
    "    if OPENWEBUI_API_KEY:\n",
    "        h[\"Authorization\"] = f\"Bearer {OPENWEBUI_API_KEY}\"\n",
    "    return h\n",
    "\n",
    "\n",
    "def call_openwebui_chat(model: str, user_message: str, system_message: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Calls OpenWebUI using an OpenAI-like chat.completions endpoint.\n",
    "    Returns assistant text.\n",
    "    \"\"\"\n",
    "    url = OPENWEBUI_BASE_URL.rstrip(\"/\") + CHAT_COMPLETIONS_PATH\n",
    "\n",
    "    messages = []\n",
    "    if system_message:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.2,\n",
    "    }\n",
    "\n",
    "    last_err = None\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            r = requests.post(url, headers=_headers(), json=payload, timeout=REQUEST_TIMEOUT_S)\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "\n",
    "            # OpenAI-like shape:\n",
    "            # data[\"choices\"][0][\"message\"][\"content\"]\n",
    "            content = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "            return content.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            if attempt < MAX_RETRIES:\n",
    "                time.sleep(1.0 * attempt)\n",
    "            else:\n",
    "                raise RuntimeError(f\"OpenWebUI call failed after {MAX_RETRIES} tries: {last_err}\") from last_err\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) JUDGE PROMPT + PARSING\n",
    "# =========================\n",
    "\n",
    "JUDGE_SYSTEM = (\n",
    "    \"Vous êtes un évaluateur strict d'un chatbot de compliance (EU AI Act). \"\n",
    "    \"Vous devez noter la réponse du modèle par rapport à la réponse de référence.\\n\\n\"\n",
    "    \"Définitions des scores (0 à 5):\\n\"\n",
    "    \"- fidelity_score: fidélité factuelle et normative par rapport à la référence (0 = faux, 5 = parfaitement fidèle)\\n\"\n",
    "    \"- quality_score: clarté, structure, actionnabilité, absence d'hallucinations, bonne mise en garde (0 = mauvais, 5 = excellent)\\n\\n\"\n",
    "    \"Répondez EXCLUSIVEMENT en JSON compact sur une seule ligne, au format:\\n\"\n",
    "    \"{\\\"fidelity_score\\\":<0-5>,\\\"quality_score\\\":<0-5>,\\\"comment\\\":\\\"...\\\"}\\n\"\n",
    ")\n",
    "\n",
    "def parse_judge_json(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract JSON from judge output (robust to extra text).\n",
    "    \"\"\"\n",
    "    # Try direct parse\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        # Extract first {...}\n",
    "        m = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
    "        if not m:\n",
    "            raise ValueError(f\"Judge output is not JSON: {text[:200]}\")\n",
    "        return json.loads(m.group(0))\n",
    "\n",
    "\n",
    "def judge_pair(reference_answer: str, model_answer: str, question: str) -> Tuple[float, float, str]:\n",
    "    user_msg = (\n",
    "        \"Évaluez la réponse du modèle.\\n\\n\"\n",
    "        f\"QUESTION:\\n{question}\\n\\n\"\n",
    "        f\"RÉPONSE DE RÉFÉRENCE:\\n{reference_answer}\\n\\n\"\n",
    "        f\"RÉPONSE DU MODÈLE:\\n{model_answer}\\n\"\n",
    "    )\n",
    "    raw = call_openwebui_chat(JUDGE_MODEL_NAME, user_msg, system_message=JUDGE_SYSTEM)\n",
    "    obj = parse_judge_json(raw)\n",
    "\n",
    "    fidelity = float(obj.get(\"fidelity_score\"))\n",
    "    quality = float(obj.get(\"quality_score\"))\n",
    "    comment = str(obj.get(\"comment\", \"\")).strip()\n",
    "    return fidelity, quality, comment\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) MAIN LOOP\n",
    "# =========================\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "    # Ensure columns exist\n",
    "    if \"question\" not in df.columns or \"reference_answer\" not in df.columns:\n",
    "        raise ValueError(\"CSV must contain columns: question, reference_answer\")\n",
    "\n",
    "    # Prepare output columns\n",
    "    if \"model_response\" not in df.columns:\n",
    "        df[\"model_response\"] = \"\"\n",
    "    if \"fidelity_score\" not in df.columns:\n",
    "        df[\"fidelity_score\"] = None\n",
    "    if \"quality_score\" not in df.columns:\n",
    "        df[\"quality_score\"] = None\n",
    "    if \"judge_comment\" not in df.columns:\n",
    "        df[\"judge_comment\"] = \"\"\n",
    "\n",
    "    # Optional: a short system prompt for your chatbot behavior\n",
    "    chatbot_system = (\n",
    "        \"Vous êtes un assistant de compliance centré sur l'EU AI Act. \"\n",
    "        \"Répondez clairement, structurez la réponse, et si vous n'êtes pas certain, dites-le.\"\n",
    "    )\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        q = str(row[\"question\"]).strip()\n",
    "        ref = str(row[\"reference_answer\"]).strip()\n",
    "\n",
    "        # Skip if already processed (useful for resume)\n",
    "        if isinstance(row.get(\"model_response\", \"\"), str) and row[\"model_response\"].strip():\n",
    "            continue\n",
    "\n",
    "        # 1) Model answer\n",
    "        model_ans = call_openwebui_chat(MODEL_NAME, q, system_message=chatbot_system)\n",
    "        df.at[i, \"model_response\"] = model_ans\n",
    "\n",
    "        # 2) Judge scores\n",
    "        fidelity, quality, comment = judge_pair(ref, model_ans, q)\n",
    "        df.at[i, \"fidelity_score\"] = fidelity\n",
    "        df.at[i, \"quality_score\"] = quality\n",
    "        df.at[i, \"judge_comment\"] = comment\n",
    "\n",
    "        # Persist at each step (safe)\n",
    "        df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "        time.sleep(SLEEP_BETWEEN_CALLS_S)\n",
    "\n",
    "    print(f\"Done. Scored CSV saved to: {OUTPUT_CSV}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
